{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47c68bb5-459c-4237-8e87-adbfd6e9df53",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 4070\n",
      "Torch version: 2.5.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n",
    "print(\"Torch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "800df281-97ee-4238-94b4-02c01c7e5e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected encoding: MacRoman\n",
      "Total rows in file: 148847\n",
      "   gvkey                                          situation\n",
      "0  64536  Euronet Worldwide Inc. has opened a new data p...\n",
      "1  16643  AMCORE Financial Inc. announced the opening of...\n",
      "2  16784  Commerce Bancorp Inc. (NJ) said that it has es...\n",
      "3  23671  Nokia Corp. boasting a fashionable new design ...\n",
      "4  16926  Mayflower Co-operative Bank announced that it ...\n",
      "Extracting location codes for all rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 148847/148847 [00:15<00:00, 9468.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting investment amounts + units for all rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 148847/148847 [00:28<00:00, 5311.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flagging climate-related events (E)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 148847/148847 [00:28<00:00, 5227.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     gvkey location invest_amount_value invest_amount_unit  E\n",
      "0    64536      HUN                None                     1\n",
      "1    16643       IL                None                     0\n",
      "2    16784       NY                None                     0\n",
      "3    23671       AL                None                     0\n",
      "4    16926       MA                None                     0\n",
      "5    16243       IL                None                     0\n",
      "6    25896       AL                None                     0\n",
      "7     2136       DE                None                     0\n",
      "8    10581      BRA                None                     0\n",
      "9    25950       FL                None                     0\n",
      "10    8446       OH                None                     0\n",
      "11  226659      JPN                None                     0\n",
      "12   29901       IL                None                     0\n",
      "13    7139     None                None                     0\n",
      "14  122062      CHN                None                     0\n",
      "15   63959       MA                None                     0\n",
      "16  120300      IND                None                     0\n",
      "17   65292       NY                None                     0\n",
      "18   13646      FRA                None                     0\n",
      "19  109919      GBR                None                     0\n",
      "Loading Sentence-BERT tokenizer...\n",
      "Loading Sentence-BERT model...\n",
      "Model loaded on device: cuda\n",
      "Computing embeddings for all situations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 148847/148847 [41:02<00:00, 60.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (148847, 768)\n",
      "Duplicate flag counts (all rows):\n",
      "dup\n",
      "0    134692\n",
      "1     14155\n",
      "Name: count, dtype: int64\n",
      "     gvkey location invest_amount_value invest_amount_unit  E  dup\n",
      "0    64536      HUN                None                     1    0\n",
      "1    16643       IL                None                     0    0\n",
      "2    16784       NY                None                     0    0\n",
      "3    23671       AL                None                     0    0\n",
      "4    16926       MA                None                     0    0\n",
      "5    16243       IL                None                     0    0\n",
      "6    25896       AL                None                     0    0\n",
      "7     2136       DE                None                     0    0\n",
      "8    10581      BRA                None                     0    0\n",
      "9    25950       FL                None                     0    0\n",
      "10    8446       OH                None                     0    0\n",
      "11  226659      JPN                None                     0    0\n",
      "12   29901       IL                None                     0    0\n",
      "13    7139     None                None                     0    0\n",
      "14  122062      CHN                None                     0    0\n",
      "15   63959       MA                None                     0    0\n",
      "16  120300      IND                None                     0    0\n",
      "17   65292       NY                None                     0    0\n",
      "18   13646      FRA                None                     0    0\n",
      "19  109919      GBR                None                     0    0\n",
      "20  100555       GA                None                     0    0\n",
      "21  234226     None                None                     0    0\n",
      "22   23252       TX                None                     0    0\n",
      "23   28262      FRA                None                     0    0\n",
      "24   25234     None                None                     0    0\n",
      "25  119314      CAN                None                     0    0\n",
      "26   28930     None                None                     0    0\n",
      "27    6829     None                None                     0    0\n",
      "28    6066       WA                None                     0    0\n",
      "29   24671     None                None                     0    0\n",
      "Saved: C:/Users/xj151/OneDrive/Desktop/1118_with_amount_location_dup.csv\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Full Pipeline:\n",
    "# 1. Multi-currency investment amount extraction (with unit + financial-report filter)\n",
    "# 2. Location extraction\n",
    "# 3. Climate-related keyword flag (E)\n",
    "# 4. Duplicate detection (Sentence-BERT)\n",
    "# Input:  global.csv with columns [\"gvkey\", \"situation\"]\n",
    "# Output: 1118_with_amount_location_dup.csv\n",
    "# ======================================================\n",
    "\n",
    "import ssl\n",
    "import chardet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import os\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Fix SSL in some Windows envs\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "# ======================================================\n",
    "# PART 1: LOCATION DICTIONARIES + extract_location_code\n",
    "# ======================================================\n",
    "\n",
    "US_STATE_PATTERNS = {\n",
    "    \"AL\": [\"alabama\", \"birmingham\", \"montgomery\", \"mobile\"],\n",
    "    \"AK\": [\"alaska\", \"anchorage\"],\n",
    "    \"AZ\": [\"arizona\", \"phoenix\", \"tucson\", \"mesa\"],\n",
    "    \"AR\": [\"arkansas\", \"little rock\", \"fayetteville\"],\n",
    "    \"CA\": [\"california\", \"los angeles\", \"san francisco\", \"san diego\", \"sacramento\", \"san jose\", \"oakland\"],\n",
    "    \"CO\": [\"colorado\", \"denver\", \"colorado springs\", \"aurora\"],\n",
    "    \"CT\": [\"connecticut\", \"hartford\", \"new haven\"],\n",
    "    \"DE\": [\"delaware\", \"wilmington\", \"dover\"],\n",
    "    \"FL\": [\"florida\", \"miami\", \"orlando\", \"tampa\", \"jacksonville\"],\n",
    "    \"GA\": [\"georgia\", \"atlanta\", \"savannah\", \"augusta\"],\n",
    "    \"HI\": [\"hawaii\", \"honolulu\"],\n",
    "    \"ID\": [\"idaho\", \"boise\"],\n",
    "    \"IL\": [\"illinois\", \"chicago\", \"springfield\", \"aurora\"],\n",
    "    \"IN\": [\"indiana\", \"indianapolis\", \"fort wayne\"],\n",
    "    \"IA\": [\"iowa\", \"des moines\"],\n",
    "    \"KS\": [\"kansas\", \"wichita\", \"topeka\"],\n",
    "    \"KY\": [\"kentucky\", \"louisville\", \"lexington\"],\n",
    "    \"LA\": [\"louisiana\", \"baton rouge\", \"new orleans\", \"lafayette\"],\n",
    "    \"ME\": [\"maine\", \"portland\", \"augusta\"],\n",
    "    \"MD\": [\"maryland\", \"baltimore\", \"rockville\"],\n",
    "    \"MA\": [\"massachusetts\", \"boston\", \"cambridge\", \"worcester\"],\n",
    "    \"MI\": [\"michigan\", \"detroit\", \"grand rapids\"],\n",
    "    \"MN\": [\"minnesota\", \"minneapolis\", \"st. paul\"],\n",
    "    \"MS\": [\"mississippi\", \"jackson\"],\n",
    "    \"MO\": [\"missouri\", \"st. louis\", \"kansas city\"],\n",
    "    \"MT\": [\"montana\", \"billings\"],\n",
    "    \"NE\": [\"nebraska\", \"omaha\", \"lincoln\"],\n",
    "    \"NV\": [\"nevada\", \"las vegas\", \"reno\"],\n",
    "    \"NH\": [\"new hampshire\", \"manchester\"],\n",
    "    \"NJ\": [\"new jersey\", \"newark\", \"jersey city\"],\n",
    "    \"NM\": [\"new mexico\", \"albuquerque\", \"santa fe\"],\n",
    "    \"NY\": [\"new york\", \"manhattan\", \"brooklyn\", \"queens\", \"buffalo\", \"rochester\"],\n",
    "    \"NC\": [\"north carolina\", \"charlotte\", \"raleigh\", \"durham\"],\n",
    "    \"ND\": [\"north dakota\", \"fargo\"],\n",
    "    \"OH\": [\"ohio\", \"columbus\", \"cleveland\", \"cincinnati\"],\n",
    "    \"OK\": [\"oklahoma\", \"oklahoma city\", \"tulsa\"],\n",
    "    \"OR\": [\"oregon\", \"portland\", \"salem\"],\n",
    "    \"PA\": [\"pennsylvania\", \"philadelphia\", \"pittsburgh\"],\n",
    "    \"RI\": [\"rhode island\", \"providence\"],\n",
    "    \"SC\": [\"south carolina\", \"charleston\", \"columbia\"],\n",
    "    \"SD\": [\"south dakota\", \"sioux falls\"],\n",
    "    \"TN\": [\"tennessee\", \"nashville\", \"memphis\", \"knoxville\"],\n",
    "    \"TX\": [\"texas\", \"houston\", \"dallas\", \"austin\", \"san antonio\", \"el paso\"],\n",
    "    \"UT\": [\"utah\", \"salt lake city\", \"provo\"],\n",
    "    \"VT\": [\"vermont\", \"burlington\"],\n",
    "    \"VA\": [\"virginia\", \"richmond\", \"virginia beach\"],\n",
    "    \"WA\": [\"washington\", \"seattle\", \"tacoma\"],\n",
    "    \"WV\": [\"west virginia\", \"charleston\"],\n",
    "    \"WI\": [\"wisconsin\", \"milwaukee\", \"madison\"],\n",
    "    \"WY\": [\"wyoming\", \"cheyenne\"],\n",
    "}\n",
    "\n",
    "COUNTRY_PATTERNS = {\n",
    "    \n",
    "    \"CAN\": [\" canada\", \" toronto\", \" vancouver\", \" lloydminster\", \" calgary\", \" montreal\"],\n",
    "    \"MEX\": [\" mexico\", \" mexico city\", \" monterrey\"],\n",
    "    \"BRA\": [\" brazil\", \" rio de janeiro\", \" sao paulo\", \" macae\"],\n",
    "    \"ARG\": [\" argentina\", \" buenos aires\"],\n",
    "    \"CHL\": [\" chile\", \" santiago\"],\n",
    "    \"COL\": [\" colombia\", \" bogota\"],\n",
    "    \"PER\": [\" peru\", \" lima\"],\n",
    "\n",
    "    \"GBR\": [\" united kingdom\", \" uk \", \" england\", \" london\", \" manchester\", \" belfast\"],\n",
    "    \"FRA\": [\" france\", \" paris\", \" lyon\"],\n",
    "    \"DEU\": [\" germany\", \" berlin\", \" munich\", \" frankfurt\"],\n",
    "    \"ITA\": [\" italy\", \" rome\", \" milan\"],\n",
    "    \"ESP\": [\" spain\", \" madrid\", \" barcelona\"],\n",
    "    \"NLD\": [\" netherlands\", \" amsterdam\"],\n",
    "    \"BEL\": [\" belgium\", \" brussels\", \" leuven\", \" louvain\"],\n",
    "    \"CHE\": [\" switzerland\", \" zurich\"],\n",
    "    \"SWE\": [\" sweden\", \" stockholm\"],\n",
    "    \"NOR\": [\" norway\", \" oslo\"],\n",
    "    \"DNK\": [\" denmark\", \" copenhagen\"],\n",
    "    \"FIN\": [\" finland\", \" helsinki\"],\n",
    "    \"IRL\": [\" ireland\", \" dublin\"],\n",
    "    \"PRT\": [\" portugal\", \" lisbon\"],\n",
    "\n",
    "    \"KOR\": [\" south korea\", \" korea \", \" seoul\", \" yeosu\"],\n",
    "    \"JPN\": [\" japan\", \" tokyo\", \" osaka\"],\n",
    "    \"CHN\": [\" china\", \" beijing\", \" shanghai\", \" shenzhen\", \" guangzhou\"],\n",
    "    \"HKG\": [\" hong kong\"],\n",
    "    \"SGP\": [\" singapore\"],\n",
    "    \"IND\": [\" india\", \" mumbai\", \" delhi\"],\n",
    "    \"PAK\": [\" pakistan\", \" karachi\"],\n",
    "    \"IDN\": [\" indonesia\", \" jakarta\"],\n",
    "    \"PHL\": [\" philippines\", \" manila\"],\n",
    "    \"VNM\": [\" vietnam\", \" hanoi\", \" ho chi minh\"],\n",
    "\n",
    "    \"AUS\": [\" australia\", \" sydney\", \" melbourne\"],\n",
    "    \"NZL\": [\" new zealand\", \" auckland\"],\n",
    "\n",
    "    \"RUS\": [\" russia\", \" moscow\", \" saint petersburg\"],\n",
    "    \"UKR\": [\" ukraine\", \" kyiv\"],\n",
    "\n",
    "    \"SAU\": [\" saudi arabia\", \" dhahran\", \" riyadh\"],\n",
    "    \"ARE\": [\" united arab emirates\", \" dubai\", \" abu dhabi\"],\n",
    "    \"QAT\": [\" qatar\", \" doha\"],\n",
    "    \"KWT\": [\" kuwait\"],\n",
    "    \"OMN\": [\" oman\", \" muscat\"],\n",
    "    \"BHR\": [\" bahrain\"],\n",
    "\n",
    "    \"ZAF\": [\" south africa\", \" johannesburg\", \" cape town\"],\n",
    "    \"EGY\": [\" egypt\", \" cairo\"],\n",
    "    \"NGA\": [\" nigeria\", \" lagos\"],\n",
    "    \"KEN\": [\" kenya\", \" nairobi\"],\n",
    "    \"GHA\": [\" ghana\", \" accra\"],\n",
    "    \"SEN\": [\" senegal\", \" taiba ndiaye\"],\n",
    "\n",
    "    \"TUR\": [\" turkey\", \" istanbul\"],\n",
    "    \"ISR\": [\" israel\", \" tel aviv\"],\n",
    "\n",
    "    \"POL\": [\" poland\", \" warsaw\"],\n",
    "    \"CZE\": [\" czech republic\", \" prague\"],\n",
    "    \"AUT\": [\" austria\", \" vienna\"],\n",
    "    \"HUN\": [\" hungary\", \" budapest\"],\n",
    "    \"GRC\": [\" greece\", \" athens\"],\n",
    "    \"ROU\": [\" romania\", \" bucharest\"],\n",
    "\n",
    "    \"THA\": [\" thailand\", \" bangkok\"],\n",
    "    \"MYS\": [\" malaysia\", \" kuala lumpur\"],\n",
    "    \"KAZ\": [\" kazakhstan\", \" almaty\"],\n",
    "    \"UZB\": [\" uzbekistan\"],\n",
    "}\n",
    "\n",
    "def extract_location_code(text: str):\n",
    "    \"\"\"\n",
    "    Return a 2-letter US state code or 3-letter country code\n",
    "    based on simple keyword matching in the text.\n",
    "    If none found, return None.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    t = \" \" + text.lower() + \" \"\n",
    "\n",
    "    # First match US states\n",
    "    for code, patterns in US_STATE_PATTERNS.items():\n",
    "        for p in patterns:\n",
    "            if p in t:\n",
    "                return code\n",
    "\n",
    "    # Then match countries\n",
    "    for code, patterns in COUNTRY_PATTERNS.items():\n",
    "        for p in patterns:\n",
    "            if p in t:\n",
    "                return code\n",
    "\n",
    "    return None\n",
    "\n",
    "# ======================================================\n",
    "# PART 2: INVESTMENT AMOUNT + UNIT + FINANCIAL REPORT FILTER\n",
    "# ======================================================\n",
    "\n",
    "INVEST_KEYWORDS = [\n",
    "    # 1) Direct investment / capital\n",
    "    \"invest\", \"investment\", \"invests\", \"invested\", \"investing\",\n",
    "    \"capital expenditure\", \"capex\", \"fund\", \"funding\", \"financing\",\n",
    "    \"finance\", \"raise capital\", \"joint venture\", \"jv\",\n",
    "    \"partnership\", \"strategic partnership\",\n",
    "\n",
    "    # 2) Expansion / new projects\n",
    "    \"expand\", \"expansion\", \"expanding\",\n",
    "    \"construct\", \"construction\", \"build\", \"building\", \"built\",\n",
    "    \"upgrade\", \"upgrading\", \"renovate\",\n",
    "    \"launch project\", \"project launch\",\n",
    "    \"project start\", \"project begins\", \"groundbreaking\",\n",
    "\n",
    "    # 3) Factory / production / capacity\n",
    "    \"factory\", \"plant\", \"facility\", \"manufacturing plant\",\n",
    "    \"production\", \"production line\",\n",
    "    \"capacity\", \"increase capacity\", \"capacity expansion\",\n",
    "    \"assembly plant\", \"manufacturing capacity\",\n",
    "    \"processing plant\", \"enrichment plant\",\n",
    "    \"smelter\", \"mill\", \"refinery\", \"mine\", \"mining project\",\n",
    "\n",
    "    # 4) R&D / technology centers\n",
    "    \"research center\", \"r&d center\", \"innovation center\",\n",
    "    \"development center\", \"engineering center\",\n",
    "    \"lab\", \"laboratory\", \"tech hub\",\n",
    "\n",
    "    # 5) Real estate / large facilities\n",
    "    \"office building\", \"campus\", \"headquarters\", \"hq\",\n",
    "    \"shopping center\", \"mall\", \"hotel\", \"resort\", \"casino\",\n",
    "    \"casino resort\", \"data center\", \"distribution center\",\n",
    "    \"logistics center\", \"warehouse\", \"fulfillment center\",\n",
    "    \"infrastructure\", \"construction project\",\n",
    "\n",
    "    # 6) New stores / branches\n",
    "    \"open store\", \"opening\", \"grand opening\",\n",
    "    \"new location\", \"new branch\", \"retail location\",\n",
    "    \"store opening\", \"store launch\",\n",
    "\n",
    "    # 7) M&A / equity investments\n",
    "    \"acquire\", \"acquisition\", \"buy\", \"merger\", \"purchase\",\n",
    "    \"takeover\", \"equity stake\", \"buyout\", \"minority stake\",\n",
    "    \"majority stake\",\n",
    "\n",
    "    # 8) Energy / resource projects\n",
    "    \"hydrogen plant\", \"power plant\", \"solar farm\", \"wind farm\",\n",
    "    \"lithium project\", \"battery plant\", \"ev plant\",\n",
    "    \"pipeline\", \"refinery expansion\",\n",
    "    \"smelting capacity\", \"mine expansion\",\n",
    "\n",
    "    # 9) Banking / financial branches\n",
    "    \"branch opening\", \"bank branch\",\n",
    "    \"opening office\", \"new office\", \"regional office\",\n",
    "]\n",
    "\n",
    "# Financial-report keyword list\n",
    "FINANCIAL_REPORT_KEYWORDS = [\n",
    "    \"financial results\",\n",
    "    \"results for the\", \"results for the first quarter\",\n",
    "    \"results for the second quarter\", \"results for the third quarter\",\n",
    "    \"results for the fourth quarter\",\n",
    "    \"quarter ended\", \"for the quarter ended\",\n",
    "    \"for the year ended\", \"fiscal year\", \"fiscal 20\",\n",
    "    \"net income\", \"net loss\",\n",
    "    \"earnings per share\", \"eps\",\n",
    "    \"profit\", \"profits\",\n",
    "    \"income\", \"operating income\", \"operating loss\",\n",
    "    \"revenue\", \"revenues\", \"net sales\", \"sales were\",\n",
    "    \"gross margin\", \"operating margin\",\n",
    "    \"ebitda\", \"adjusted ebitda\",\n",
    "    \"cash flow\", \"cash flows from operations\",\n",
    "    \"balance sheet\", \"statement of operations\",\n",
    "    \"consolidated financial statements\",\n",
    "    \"dividend\", \"dividends per share\",\n",
    "]\n",
    "\n",
    "# Multi-currency symbols\n",
    "CURRENCY_SYMBOLS = \"$€¥￥£\"\n",
    "\n",
    "# Range like \"$1,000 to $2,000\"\n",
    "RANGE_PATTERN = re.compile(\n",
    "    rf\"([{CURRENCY_SYMBOLS}])\\s*(\\d[\\d,]*)\\s*(to|-)\\s*\\1\\s*(\\d[\\d,]*)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Single amount with optional unit after the number: $100 million, €2.5bn, ¥300k, etc.\n",
    "CURRENCY_PATTERN = re.compile(\n",
    "    rf\"([{CURRENCY_SYMBOLS}])\\s*(\\d[\\d,]*(?:\\.\\d+)?)\\s*\"\n",
    "    r\"(million|billion|thousand|m|bn|k|crore|lakh)?\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def simple_sent_tokenize(text: str):\n",
    "    \"\"\"Very simple sentence splitter based on punctuation.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    parts = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def clean_num(s: str) -> float:\n",
    "    \"\"\"Remove commas and convert to float.\"\"\"\n",
    "    return float(s.replace(\",\", \"\"))\n",
    "\n",
    "def is_financial_report(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic: if the text looks like an earnings/financial report\n",
    "    (multiple mentions of financial-report-related phrases),\n",
    "    we treat the whole text as a non-investment context.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return False\n",
    "    t = text.lower()\n",
    "    hits = 0\n",
    "    for kw in FINANCIAL_REPORT_KEYWORDS:\n",
    "        if kw in t:\n",
    "            hits += 1\n",
    "            if hits >= 2:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def extract_amount_from_sentence(sent: str):\n",
    "    \"\"\"\n",
    "    Extract amount and unit from a single sentence:\n",
    "    - Only consider sentences containing at least one INVEST_KEYWORD.\n",
    "    - Look for currency amounts (with symbols).\n",
    "    - If a range like \"$1000 to $2000\" is found, return (range_string, \"\").\n",
    "    - Otherwise, sum all numbers after currency symbols in this sentence\n",
    "      and infer a common unit if possible.\n",
    "    Returns:\n",
    "        None         -> no valid investment amount\n",
    "        (value, unit)\n",
    "            - value: float (summed) OR str (for range)\n",
    "            - unit:  unit string like 'million', 'bn', 'k', etc., or \"\" if none.\n",
    "    \"\"\"\n",
    "    s_lower = sent.lower()\n",
    "\n",
    "    # Must look like an investment-related sentence\n",
    "    if not any(kw in s_lower for kw in INVEST_KEYWORDS):\n",
    "        return None\n",
    "\n",
    "    # First, check for explicit range \"$1000 to $2000\"\n",
    "    rng_match = RANGE_PATTERN.search(sent)\n",
    "    if rng_match:\n",
    "        # Keep the full range string as requested, unit left blank\n",
    "        return rng_match.group(0), \"\"\n",
    "\n",
    "    # Otherwise, collect all currency amounts in this sentence\n",
    "    matches = CURRENCY_PATTERN.findall(sent)\n",
    "    if not matches:\n",
    "        return None\n",
    "\n",
    "    total = 0.0\n",
    "    units = []\n",
    "\n",
    "    for symbol, num_str, unit in matches:\n",
    "        total += clean_num(num_str)\n",
    "        if unit:\n",
    "            units.append(unit.lower())\n",
    "\n",
    "    # Decide the unit for this sentence:\n",
    "    # if all non-empty units are the same, keep it; else, use \"\"\n",
    "    sentence_unit = \"\"\n",
    "    if units:\n",
    "        unique_units = set(units)\n",
    "        if len(unique_units) == 1:\n",
    "            sentence_unit = unique_units.pop()\n",
    "\n",
    "    return total, sentence_unit\n",
    "\n",
    "def extract_amount_and_unit(text: str):\n",
    "    \"\"\"\n",
    "    Main entry for amount + unit extraction from a full text:\n",
    "    1) If it's a financial report → return (None, \"\").\n",
    "    2) Else, split into sentences and run sentence-level extraction.\n",
    "       - Keep the largest numeric amount across sentences.\n",
    "       - Keep the first range string if any (and prefer range over numeric).\n",
    "    Returns:\n",
    "        (value, unit)\n",
    "        - value: float or str (for range), or None\n",
    "        - unit:  unit string or \"\" if no unit\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return None, \"\"\n",
    "\n",
    "    if is_financial_report(text):\n",
    "        # Financial report → ignore as investment\n",
    "        return None, \"\"\n",
    "\n",
    "    sentences = simple_sent_tokenize(text)\n",
    "\n",
    "    best_numeric = None\n",
    "    best_numeric_unit = \"\"\n",
    "\n",
    "    best_range = None\n",
    "    best_range_unit = \"\"\n",
    "\n",
    "    for sent in sentences:\n",
    "        res = extract_amount_from_sentence(sent)\n",
    "        if res is None:\n",
    "            continue\n",
    "\n",
    "        val, unit = res\n",
    "\n",
    "        # Range case: keep the first range encountered\n",
    "        if isinstance(val, str):\n",
    "            if best_range is None:\n",
    "                best_range = val\n",
    "                best_range_unit = unit or \"\"\n",
    "        else:\n",
    "            # Numeric case: keep the largest numeric value\n",
    "            if best_numeric is None or val > best_numeric:\n",
    "                best_numeric = val\n",
    "                best_numeric_unit = unit or \"\"\n",
    "\n",
    "    # If there is any range, prefer it\n",
    "    if best_range is not None:\n",
    "        return best_range, best_range_unit\n",
    "\n",
    "    return best_numeric, best_numeric_unit\n",
    "\n",
    "# ======================================================\n",
    "# PART 2.5: CLIMATE / ENVIRONMENT KEYWORDS  ->  E flag\n",
    "# ======================================================\n",
    "\n",
    "CLIMATE_KEYWORDS_RAW = \"\"\"\n",
    "renewable energy,electric vehicle,clean energy,new energy,climate,energy,natural gas,hydrogen,natural disaster,\n",
    "wind power,wind energy,energy efficient,greenhouse gas,solar energy,\n",
    "air quality,clean air,carbon emission,gas emission,extreme weather,emission,carbon,\n",
    "carbon dioxide,water resource,autonomous vehicle,energy environment,\n",
    "wind resource,government india,battery power,air pollution,battery electric,\n",
    "integrate resource,clean power,carbon price,world population,solar farm,\n",
    "energy regulatory,obama administration,heat power,carbon tax,unite nation,\n",
    "onshore wind,electric motor,provide energy,efficient solution,global warm,\n",
    "power generator,solar pv,scale solar,need clean,coastal area,energy star,\n",
    "environmental footprint,design use,area energy,charge station,clean water,\n",
    "major design,vehicle manufacturer,motor control,combine heat,electric bus,\n",
    "distribute power,environmental benefit,eco friendly,electrical vehicle,\n",
    "carbon neutral,fast charge,cell power,energy team,cycle gas,\n",
    "coal gasification,environmental concern,carbon intensity,energy application,\n",
    "produce electricity,help state,environmental standard,power agreement,\n",
    "supply energy,electric hybrid,source power,sustainability goal,energy reform,\n",
    "plant power,compare conventional,gas vehicle,effort energy,pass house,\n",
    "carbon free,driver assistance,electrical energy,solar installation,snow ice,\n",
    "renewable natural,promote use,farm project,laser diode,deliver energy,\n",
    "protect environment,sustainable energy,manage energy,invest energy,\n",
    "electric energy,forest land,capacity energy,unite nation,\n",
    "hurricane,hurricanes,storms,drought,flooding,flood,wildfire,wildfires,\n",
    "windstorm,storm losses,severe winter,storm related,storm activity,polar vortex,\n",
    "storm season,storm damage,droughts,tropical storm,snowstorms,winter storm,\n",
    "hailstorm,volcano,extreme cold,cold winter,heat wave,heating season,\n",
    "precipitation,ice season,snowfall,rainfall,degree days,winter conditions,\n",
    "warm winter,heavy rains,cold summer,unseasonably warm,harsh winter,clouds,\n",
    "the warmest,early winter,the warm,cool summer,cold wind,heavy winter,\n",
    "the rain,the winds,energy efficiency,alternative energy,superior energy,higher energy,\n",
    "renewable,ecosystem,energy management,the carbon,green energy,\n",
    "shale gas,fuel efficient,solar power,battery,wind farm,\n",
    "fuel economy,solar cell,energy future,solar projects,decarbonization,\n",
    "emissions,climate risk,climate change,biodiversity,low carbon,\n",
    "\n",
    "decarbon,decarbonize,decarbonized,decarbonization,\n",
    "net zero,net-zero,carbon neutrality,\n",
    "emission reduce,emission reduction,reduce emission,lower emission,\n",
    "low-carbon,carbon footprint,GHG emission,greenhouse gas emission,\n",
    "methane leak,methane emission,CH4 emission,\n",
    "climate-related risk,climate resilience,resilience plan,\n",
    "physical risk,transition risk,environmental risk,sustainability risk,ESG risk,\n",
    "environmental sustainability,corporate sustainability,sustainable finance,\n",
    "energy transition,transition energy,transition plan,green transition,\n",
    "renewable project,renewable capacity,renewable expansion,\n",
    "clean technology,cleantech,green technology,sustainable technology,\n",
    "green innovation,eco innovation,eco-friendly,environmentally friendly,\n",
    "biofuel,biodiesel,biogas,bioenergy,\n",
    "hydrogen energy,green hydrogen,blue hydrogen,H2 production,hydrogen project,\n",
    "carbon capture,CCS,carbon capture storage,CCUS,carbon sequestration,\n",
    "direct air capture,DAC,air capture technology,\n",
    "energy storage,battery storage,grid storage,\n",
    "sea level rise,coastal flooding,coastal erosion,\n",
    "environmental regulation,EPA regulation,emission regulation,\n",
    "climate regulation,carbon regulation,emission standard,\n",
    "carbon pricing,carbon market,emission trading,cap and trade,\n",
    "pollution control,water pollution,soil contamination,\n",
    "environmental impact,environmental compliance,environmental penalty,\n",
    "sustainable material,recycled material,eco material,\n",
    "green building,LEED certified,energy-efficient building,\n",
    "energy conservation,efficiency upgrade,\n",
    "solar panel,solar capacity,solar generation,\n",
    "wind turbine,wind installation,wind capacity,\n",
    "geothermal energy,tidal energy,wave energy,\n",
    "EV charging,charging infrastructure,EV infrastructure,\n",
    "electric mobility,clean mobility,zero emission vehicle,\n",
    "sustainable supply chain,green supply chain,low-carbon supply chain,\n",
    "circular economy,recycling program,waste reduction,waste management,\n",
    "nature-based solution,forest conservation,reforestation,afforestation,\n",
    "ecosystem service,habitat restoration,\n",
    "paris agreement,paris climate agreement,climate summit,\n",
    "science based target,science-based target,\n",
    "\n",
    "GHG protocol,greenhouse inventory,emission inventory,\n",
    "carbon offset,carbon offsets,carbon credit,carbon credits,carbon trading,\n",
    "carbon removal,carbon removals,negative emission,negative emissions,\n",
    "REDD+,avoided deforestation,deforestation free,zero deforestation,\n",
    "climate mitigation,mitigation measure,mitigation strategy,\n",
    "climate adaptation,adaptation measure,adaptation strategy,\n",
    "climate scenario,two degree scenario,temperature pathway,\n",
    "low-carbon transition,net-zero transition,transition pathway,\n",
    "resilient infrastructure,climate-resilient,weather-resilient,\n",
    "hardening infrastructure,grid hardening,storm hardening,\n",
    "flood defence,flood wall,sea wall,levee system,\n",
    "microgrid,smart grid,grid modernization,grid upgrade,grid resilience,\n",
    "demand response,peak shaving,load shifting,flexible load,\n",
    "heat pump,heat pumps,district heating,district cooling,\n",
    "building retrofit,deep retrofit,energy retrofit,\n",
    "high-efficiency appliance,efficient appliance,high efficiency motor,\n",
    "vehicle electrification,transport electrification,fleet electrification,\n",
    "zero-emission truck,zero-emission bus,electric fleet,\n",
    "green logistics,low-carbon logistics,clean logistics,\n",
    "sustainable aviation fuel,low carbon fuel standard,\n",
    "fuel switching,coal-to-gas,coal phase-out,coal phase down,\n",
    "stranded asset,carbon-intensive asset,high-carbon asset,\n",
    "climate disclosure,TCFD,climate reporting,climate-related disclosure,\n",
    "ESG report,sustainability report,non-financial reporting,\n",
    "green bond,climate bond,sustainability-linked bond,SLB,\n",
    "green loan,sustainability-linked loan,\n",
    "impact investing,climate finance,green finance,\n",
    "nature risk,nature-related risk,TNFD,\n",
    "natural capital,ecosystem capital,ecosystem resilience,\n",
    "land use change,land-use change,LULUCF,\n",
    "peatland,mangrove,wetland restoration,\n",
    "water stress,water scarcity,water shortage,water risk,\n",
    "drought resistant,drought tolerant,water saving technology,\n",
    "storm surge,coastal storm,tidal surge,\n",
    "urban heat island,heat island effect,\n",
    "air quality index,AQI,fine particulate,PM2.5,PM10,\n",
    "nitrogen oxide,NOx emission,sulfur dioxide,SO2 emission,ozone pollution,smog episode,\n",
    "waste heat recovery,heat recovery system,\n",
    "life cycle assessment,life-cycle assessment,LCA study,\n",
    "cradle to grave,cradle-to-cradle,\n",
    "eco design,ecodesign,design for environment,\n",
    "hazardous waste,toxic release,chemical spill,\n",
    "sustainable agriculture,climate smart agriculture,regenerative agriculture,\n",
    "organic farming,soil carbon,soil sequestration,\n",
    "reforestation project,tree planting program,tree plantation,\n",
    "water stewardship,responsible water use,\n",
    "green procurement,sustainable procurement,responsible sourcing,\n",
    "low-impact material,low impact material,\n",
    "biodegradable material,compostable material,\n",
    "ocean plastic,plastic waste reduction,single-use plastic phase-out,\n",
    "zero waste,landfill diversion,waste diversion,\n",
    "renewable portfolio standard,RPS target,\n",
    "feed-in tariff,renewable incentive,green tariff,\n",
    "time-of-use pricing,dynamic pricing,\n",
    "vehicle-to-grid,V2G,\n",
    "energy benchmark,efficiency benchmark,\n",
    "climate lawsuit,climate litigation,\n",
    "environmental justice,climate justice,\n",
    "just transition,worker transition plan,\n",
    "eco-label,environmental label,energy label\n",
    "\"\"\"\n",
    "\n",
    "def build_climate_keywords(raw: str):\n",
    "    tokens = re.split(r'[,\\|，]', raw)\n",
    "    return sorted({t.strip().lower() for t in tokens if t.strip()})\n",
    "\n",
    "CLIMATE_KEYWORDS = build_climate_keywords(CLIMATE_KEYWORDS_RAW)\n",
    "\n",
    "def has_climate_keyword(text: str) -> int:\n",
    "    \"\"\"\n",
    "   \n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0\n",
    "    t = text.lower()\n",
    "    for kw in CLIMATE_KEYWORDS:\n",
    "        if kw in t:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# ======================================================\n",
    "# PART 3: LOAD CSV\n",
    "# ======================================================\n",
    "\n",
    "events_file_path = \"C:/Users/xj151/OneDrive/Desktop/global.csv\"\n",
    "\n",
    "with open(events_file_path, \"rb\") as f:\n",
    "    enc = chardet.detect(f.read())[\"encoding\"]\n",
    "print(\"Detected encoding:\", enc)\n",
    "\n",
    "df = pd.read_csv(events_file_path, encoding=enc, on_bad_lines=\"skip\", engine=\"python\")\n",
    "df = df.copy().reset_index(drop=True)\n",
    "\n",
    "required_cols = [\"gvkey\", \"situation\"]\n",
    "for col in required_cols:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column '{col}' not found in input CSV\")\n",
    "\n",
    "df[\"situation\"] = df[\"situation\"].fillna(\"\")\n",
    "df[\"gvkey\"] = df[\"gvkey\"].fillna(\"UNKNOWN\")\n",
    "\n",
    "print(\"Total rows in file:\", len(df))\n",
    "print(df[[\"gvkey\", \"situation\"]].head())\n",
    "\n",
    "# ======================================================\n",
    "# PART 4: LOCATION + AMOUNT+UNIT + CLIMATE FLAG (E)\n",
    "# ======================================================\n",
    "\n",
    "print(\"Extracting location codes for all rows...\")\n",
    "df[\"location\"] = df[\"situation\"].progress_apply(extract_location_code)\n",
    "\n",
    "print(\"Extracting investment amounts + units for all rows...\")\n",
    "df[[\"invest_amount_value\", \"invest_amount_unit\"]] = df[\"situation\"].progress_apply(\n",
    "    lambda x: pd.Series(extract_amount_and_unit(x))\n",
    ")\n",
    "\n",
    "print(\"Flagging climate-related events (E)...\")\n",
    "df[\"E\"] = df[\"situation\"].progress_apply(has_climate_keyword)\n",
    "\n",
    "print(df[[\"gvkey\", \"location\", \"invest_amount_value\", \"invest_amount_unit\", \"E\"]].head(20))\n",
    "\n",
    "# ======================================================\n",
    "# PART 5: Sentence-BERT Duplicate Detection\n",
    "# ======================================================\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "print(\"Loading Sentence-BERT tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Loading Sentence-BERT model...\")\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded on device:\", device)\n",
    "\n",
    "def get_sentence_embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute a sentence embedding using:\n",
    "    - Transformer last hidden states\n",
    "    - Attention-mask-based mean pooling (ignore padding)\n",
    "    - L2 normalization\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return np.zeros(model.config.hidden_size, dtype=np.float32)\n",
    "\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=True\n",
    "    )\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(**enc)\n",
    "\n",
    "    last_hidden = out.last_hidden_state         # (1, seq_len, hidden)\n",
    "    mask = enc[\"attention_mask\"].unsqueeze(-1)  # (1, seq_len, 1)\n",
    "\n",
    "    masked = last_hidden * mask\n",
    "    summed = masked.sum(dim=1)                  # (1, hidden)\n",
    "    lengths = mask.sum(dim=1)                   # (1, 1)\n",
    "    emb = (summed / lengths).squeeze(0).cpu().numpy()\n",
    "\n",
    "    norm = np.linalg.norm(emb)\n",
    "    if norm > 0:\n",
    "        emb = emb / norm\n",
    "    return emb.astype(np.float32)\n",
    "\n",
    "print(\"Computing embeddings for all situations...\")\n",
    "emb_list = df[\"situation\"].progress_apply(get_sentence_embedding).values\n",
    "emb_matrix = np.vstack(emb_list)\n",
    "print(\"Embedding matrix shape:\", emb_matrix.shape)\n",
    "\n",
    "# ======================================================\n",
    "# PART 6: DUPLICATE FLAG (dup) — group by gvkey\n",
    "# ======================================================\n",
    "\n",
    "SIM_THRESHOLD = 0.9\n",
    "dup_flags = np.zeros(len(df), dtype=int)\n",
    "\n",
    "for gv, g in df.groupby(\"gvkey\"):\n",
    "    idx_list = list(g.index)\n",
    "    rep_indices = []\n",
    "\n",
    "    for i in idx_list:\n",
    "        loc_i = df.loc[i, \"location\"]\n",
    "        emb_i = emb_matrix[i].reshape(1, -1)\n",
    "\n",
    "        if not rep_indices:\n",
    "            dup_flags[i] = 0\n",
    "            rep_indices.append(i)\n",
    "            continue\n",
    "\n",
    "        candidate_indices = []\n",
    "        for j in rep_indices:\n",
    "            loc_j = df.loc[j, \"location\"]\n",
    "            if (pd.notna(loc_i)) and (pd.notna(loc_j)) and (loc_i != loc_j):\n",
    "                continue\n",
    "            candidate_indices.append(j)\n",
    "\n",
    "        if not candidate_indices:\n",
    "            dup_flags[i] = 0\n",
    "            rep_indices.append(i)\n",
    "            continue\n",
    "\n",
    "        rep_embs = emb_matrix[candidate_indices]\n",
    "        sims = cosine_similarity(emb_i, rep_embs)[0]\n",
    "        max_sim = sims.max()\n",
    "\n",
    "        if max_sim >= SIM_THRESHOLD:\n",
    "            dup_flags[i] = 1\n",
    "        else:\n",
    "            dup_flags[i] = 0\n",
    "            rep_indices.append(i)\n",
    "\n",
    "df[\"dup\"] = dup_flags\n",
    "\n",
    "print(\"Duplicate flag counts (all rows):\")\n",
    "print(df[\"dup\"].value_counts())\n",
    "print(df[[\"gvkey\", \"location\", \"invest_amount_value\", \"invest_amount_unit\", \"E\", \"dup\"]].head(30))\n",
    "\n",
    "# ======================================================\n",
    "# SAVE RESULT\n",
    "# ======================================================\n",
    "\n",
    "output_path = \"C:/Users/xj151/OneDrive/Desktop/1118_with_amount_location_dup.csv\"\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "print(\"Saved:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a5d2e4-f5a0-44ea-b63f-11d6799b1367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds1975)",
   "language": "python",
   "name": "ds1975"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
